{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "df = pd.read_csv(\"../data/processed/labelled_texts.csv\")\n",
    "df = df[['complaint_what_happened', 'topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio split: 80.0%\n",
      "Validation ratio split: 10.0%\n",
      "Test ratio split: 10.0%\n"
     ]
    }
   ],
   "source": [
    "# Split data:\n",
    "X = df['complaint_what_happened'].to_numpy()\n",
    "y = df['topic'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, stratify=y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train ratio split: {X_train.shape[0]/ len(df):.1%}\")\n",
    "print(f\"Validation ratio split: {X_val.shape[0]/ len(df):.1%}\")\n",
    "print(f\"Test ratio split: {X_test.shape[0]/ len(df):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions for Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove punctuations from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed punctuations.\n",
    "    \"\"\"\n",
    "    pattern = f'[{re.escape(string.punctuation)}]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_numbers(text: str) -> str:\n",
    "    \"\"\"Remove numbers from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with numbers punctuations.\n",
    "    \"\"\"\n",
    "    pattern = r'[0-9]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_confidential_information(text: str) -> str:\n",
    "    \"\"\"Remove confidential information from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed confidential information.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b[Xx]{1,}\\b'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text: str) -> str:\n",
    "    \"\"\"Remove extra spaces or new lines from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed extra spaces or new lines.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s+'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"Remove stop words from text\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        str: text with stop words removed\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    return ' '.join([token for token in tokens if token not in stop_words])\n",
    "\n",
    "# source: https://www.ibm.com/topics/stemming-lemmatization#:~:text=The%20practical%20distinction%20between%20stemming,be%20found%20in%20the%20dictionary.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    \"\"\"Return wordnet constant value to do lemmatization based on their input word tag\n",
    "\n",
    "    Args:\n",
    "        tag (str): tag name\n",
    "\n",
    "    Returns:\n",
    "        str: constant value for wordnet lemmatization\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"Perform lemmatization using WordNetLemmatizer\n",
    "\n",
    "    Args:\n",
    "        tokens (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    return ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text preprocessing pipeline\n",
    "def process_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Text cleaning: \n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_confidential_information(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "\n",
    "    # Text transformation:\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5238, 0.8898, 0.6027, 0.1141, 0.7992, 0.9660, 0.4937, 0.2413, 0.0340,\n",
       "         0.8421, 0.5325, 0.6562, 0.7827, 0.2183, 0.7477, 0.7465, 0.9991, 0.8233,\n",
       "         0.5981, 0.4954, 0.3899]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((1, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mapping for label\n",
    "label_to_ix = {\n",
    "    'Bank Account services': 0, \n",
    "    'Credit card or prepaid card': 1,\n",
    "    'Mortgage/Loan': 2, \n",
    "    'Theft/Dispute Reporting': 3, \n",
    "    'Others': 4\n",
    "}\n",
    "\n",
    "ix_to_label = {\n",
    "    0: 'Bank Account services', \n",
    "    1: 'Credit card or prepaid card',\n",
    "    2: 'Mortgage/Loan', \n",
    "    3: 'Theft/Dispute Reporting', \n",
    "    4: 'Others'\n",
    "}\n",
    "\n",
    "def encode_label(label: list):\n",
    "    return label_to_ix[label]\n",
    "\n",
    "def encode_text(text: str, word_to_ix: dict):\n",
    "    return [word_to_ix.get(word, 0) for word in tokenizer(text)]\n",
    "\n",
    "\n",
    "class CustomerComplaintsDataset(Dataset):\n",
    "    def __init__(self, encoded_texts: list, encoded_labels: list, max_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def padding(self, tokens: list, max_length: int):\n",
    "        if len(tokens) < max_length:\n",
    "            zeros = list(np.zeros(max_length - len(tokens)))\n",
    "            new = tokens + zeros\n",
    "        else:\n",
    "            new = tokens[:max_length]\n",
    "        features = np.array(new)\n",
    "        return features \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = self.padding(self.encoded_texts[idx], self.max_length)\n",
    "        label = self.encoded_labels[idx]\n",
    "        return torch.LongTensor(encoded_text), label\n",
    "    \n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     texts, labels = zip(*batch)\n",
    "    \n",
    "#     # Pad sequences to the same length\n",
    "#     padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "#     return padded_texts, torch.tensor(labels, dtype=int)\n",
    "\n",
    "\n",
    "# Combine it all\n",
    "def text_preprocessing_pipeline(texts, labels):\n",
    "    # Process the text\n",
    "    processed_texts = list(map(process_text, texts))\n",
    "    \n",
    "    # Generate vocabulary: \n",
    "    vocabulary = set(' '.join(processed_texts).split())\n",
    "    word_to_ix = {vocab: ix+1 for ix, vocab in enumerate(vocabulary)}\n",
    "    \n",
    "    # Calculate maximum length\n",
    "    max_length = -1  \n",
    "    for text in processed_texts:\n",
    "        max_length = max(max_length, len(tokenizer(text)))\n",
    "    \n",
    "\n",
    "    # Encode texts:\n",
    "    encoded_texts = [encode_text(text, word_to_ix) for text in processed_texts] \n",
    "    # Encode labels:\n",
    "    encoded_labels = [encode_label(label) for label in labels]\n",
    "    dataset = CustomerComplaintsDataset(encoded_texts, encoded_labels, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    return dataloader, vocabulary, max_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, vocabulary, max_length = text_preprocessing_pipeline(df['complaint_what_happened'].tolist()[:500], df['topic'].tolist()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 2843])\n",
      "torch.Size([64])\n",
      "torch.Size([52, 2843])\n",
      "torch.Size([52])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in dataloader:\n",
    "    print(features.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary) + 1\n",
    "embedding_size = 128\n",
    "num_classes = len(df['topic'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_size * max_length, num_classes)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.embedding, \n",
    "            self.flatten, \n",
    "            self.fc\n",
    "        )\n",
    "        return model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model hyperparameters\n",
    "vocab_size = len(vocabulary) + 1\n",
    "embedding_size = 128\n",
    "num_classes = len(df['topic'].unique())\n",
    "\n",
    "# Create model\n",
    "model = Net(vocab_size, embedding_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for texts, labels in dataloader:\n",
    "        # Reset gradients:\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Produce model output\n",
    "        outputs = model(texts)\n",
    "\n",
    "        # loss model\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2055.0043954849243,\n",
       " 906.358283996582,\n",
       " 625.0599822998047,\n",
       " 395.15110969543457,\n",
       " 324.28125190734863,\n",
       " 220.6447949409485,\n",
       " 100.28202056884766,\n",
       " 95.68417572975159,\n",
       " 142.01598179340363,\n",
       " 39.23400700092316]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-complaints-classification-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
