{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning my name is XXXX XXXX and I apprec...</td>\n",
       "      <td>Bank Account services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chase Card was reported on XX/XX/2019. However...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On XX/XX/2018, while trying to book a XXXX  XX...</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my grand son give me check for {$1600.00} i de...</td>\n",
       "      <td>Bank Account services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             complaint_what_happened  \\\n",
       "0  Good morning my name is XXXX XXXX and I apprec...   \n",
       "1  I upgraded my XXXX XXXX card in XX/XX/2018 and...   \n",
       "2  Chase Card was reported on XX/XX/2019. However...   \n",
       "3  On XX/XX/2018, while trying to book a XXXX  XX...   \n",
       "4  my grand son give me check for {$1600.00} i de...   \n",
       "\n",
       "                         topic  \n",
       "0        Bank Account services  \n",
       "1  Credit card or prepaid card  \n",
       "2  Credit card or prepaid card  \n",
       "3  Credit card or prepaid card  \n",
       "4        Bank Account services  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../data/processed/labelled_texts.csv\")\n",
    "df = df[['complaint_what_happened', 'topic']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up map for label\n",
    "label_to_id = {\n",
    "    'Bank Account services': 0, \n",
    "    'Credit card or prepaid card': 1,\n",
    "    'Mortgage/Loan': 2, \n",
    "    'Theft/Dispute Reporting': 3, \n",
    "    'Others': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions for Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def remove_numbers_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove numbers and punctuations from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed punctuations.\n",
    "    \"\"\"\n",
    "    pattern = r'[^a-zA-Z]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_word_numbers(text: str) -> str:\n",
    "    \"\"\"Remove punctuations from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed punctuations.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b\\w*\\d\\w*\\b'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_confidential_information(text: str) -> str:\n",
    "    \"\"\"Remove confidential information from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed confidential information.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b[Xx]{1,}\\b'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text: str) -> str:\n",
    "    \"\"\"Remove extra spaces or new lines from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): text.\n",
    "    Returns:\n",
    "        str: text with removed extra spaces or new lines.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s+'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"Remove stop words from text\n",
    "\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        str: text with stop words removed\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([token for token in tokens if token not in stop_words])\n",
    "\n",
    "# source: https://www.ibm.com/topics/stemming-lemmatization#:~:text=The%20practical%20distinction%20between%20stemming,be%20found%20in%20the%20dictionary.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    \"\"\"Return wordnet constant value to do lemmatization based on their input word tag\n",
    "\n",
    "    Args:\n",
    "        tag (str): tag name\n",
    "\n",
    "    Returns:\n",
    "        str: constant value for wordnet lemmatization\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"Perform lemmatization using WordNetLemmatizer\n",
    "\n",
    "    Args:\n",
    "        tokens (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags])\n",
    "\n",
    "def preprocess_texts(texts: list) -> list:\n",
    "    processed_texts = list()\n",
    "    vocabulary = set()\n",
    "    max_length = -1\n",
    "    for text in texts:\n",
    "        try:\n",
    "            text = remove_word_numbers(text)  # Remove word containing numbers\n",
    "            text = remove_numbers_punctuations(text)  # Remove punctuations and numbers\n",
    "            text = remove_confidential_information(text)  # Remove confidential information\n",
    "            text = remove_extra_spaces(text)  # Remove extra spaces\n",
    "            text = text.lower()  # Convert to lower case letters\n",
    "            text = remove_stopwords(text)  # Remove stop words\n",
    "            text = lemmatize(text)  # Perform lemmatization\n",
    "            processed_texts.append(text)  # Append text to processed_texts list\n",
    "            vocabulary.update(text.split())  # Append words to vocabulary\n",
    "            max_length = max(max_length, len(text.split()))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(text)\n",
    "    \n",
    "    # Word to index mapping\n",
    "    vocabulary = sorted(vocabulary)\n",
    "    return processed_texts, vocabulary, max_length\n",
    "\n",
    "def encode_texts(texts: list, word_to_idx: dict):\n",
    "    encoded_texts = []\n",
    "    for text in texts:\n",
    "        encoded_texts.append([word_to_idx.get(word, 0) for word in text.split()])\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up map for label encoding\n",
    "label_to_id = {\n",
    "    'Bank Account services': 0, \n",
    "    'Credit card or prepaid card': 1,\n",
    "    'Mortgage/Loan': 2, \n",
    "    'Theft/Dispute Reporting': 3, \n",
    "    'Others': 4\n",
    "}\n",
    "\n",
    "\n",
    "def encode_labels(labels: list):\n",
    "    return [label_to_id[label] for label in labels]\n",
    "\n",
    "\n",
    "class CustomerComplaintsDataset(Dataset):\n",
    "    def __init__(self, texts: list, encoded_labels: list, vocabulary: set, max_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.word_to_idx = {word: idx+1 for idx, word in enumerate(vocabulary)}\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def padding(self, tokens: list):\n",
    "        if len(tokens) < self.max_length:\n",
    "            zeros = list(np.zeros(self.max_length - len(tokens)))\n",
    "            new = tokens + zeros\n",
    "        else:\n",
    "            new = tokens[:self.max_length]\n",
    "        features = np.array(new)\n",
    "        return features \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_text = [self.word_to_idx.get(word, 0) for word in self.texts[idx].split()]\n",
    "        encoded_text = self.padding(encoded_text)\n",
    "        label = self.encoded_labels[idx]\n",
    "        return torch.LongTensor(encoded_text), label\n",
    "    \n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the same length\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    return padded_texts, torch.tensor(labels, dtype=int)\n",
    "\n",
    "\n",
    "# Combine it all\n",
    "def text_preprocessing_pipeline(texts, labels):\n",
    "    processed_texts, vocabulary, max_length = preprocess_texts(texts)\n",
    "    encoded_labels = encode_labels(labels)\n",
    "    dataset = CustomerComplaintsDataset(processed_texts, encoded_labels, vocabulary, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader, vocabulary, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, vocabulary, max_length = text_preprocessing_pipeline(df['complaint_what_happened'].tolist()[:1000], df['topic'].tolist()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary) + 1\n",
    "embedding_size = 32\n",
    "num_class = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerComplaintClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes):\n",
    "        super(CustomerComplaintClassifier, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc = torch.nn.Linear(embedding_size * max_length, num_classes)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.embedding, \n",
    "            self.flatten, \n",
    "            self.fc\n",
    "        )\n",
    "        return model(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomerComplaintClassifier(vocab_size, embedding_size, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "lr = 1e-3\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    for texts, labels in dataloader:\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2867, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-complaints-classification-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
