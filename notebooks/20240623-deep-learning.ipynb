{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awmarcel/miniconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/awmarcel/miniconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/awmarcel/miniconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data manipulation:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "# Data splitting:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning Modelling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Text preprocessing\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Evaluation metrics:\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Recall\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "df = pd.read_csv(\"../data/processed/labelled_texts.csv\")\n",
    "df = df[['complaint_what_happened', 'topic']]\n",
    "\n",
    "# Set label to index mapping and vice versa:\n",
    "label_to_ix = {\n",
    "    'Bank Account services': 0, \n",
    "    'Credit card or prepaid card': 1,\n",
    "    'Mortgage/Loan': 2, \n",
    "    'Theft/Dispute Reporting': 3, \n",
    "    'Others': 4\n",
    "}\n",
    "\n",
    "ix_to_label = {\n",
    "    0: 'Bank Account services', \n",
    "    1: 'Credit card or prepaid card',\n",
    "    2: 'Mortgage/Loan', \n",
    "    3: 'Theft/Dispute Reporting', \n",
    "    4: 'Others'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ratio split: 80.0%\n",
      "Validation ratio split: 10.0%\n",
      "Test ratio split: 10.0%\n"
     ]
    }
   ],
   "source": [
    "# Split data:\n",
    "X = df['complaint_what_happened'].tolist()\n",
    "y = df['topic'].tolist()\n",
    "\n",
    "# Split data into training, validation, and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, stratify=y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train ratio split: {len(X_train)/ len(df):.1%}\")\n",
    "print(f\"Validation ratio split: {len(X_val)/ len(df):.1%}\")\n",
    "print(f\"Test ratio split: {len(X_test)/ len(df):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions for Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Functions\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove punctuations from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text.\n",
    "    Returns:\n",
    "        str: Text with removed punctuations.\n",
    "    \"\"\"\n",
    "    pattern = f'[{re.escape(string.punctuation)}]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_numbers(text: str) -> str:\n",
    "    \"\"\"Remove numbers from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text.\n",
    "    Returns:\n",
    "        str: Text with numbers punctuations.\n",
    "    \"\"\"\n",
    "    pattern = r'[0-9]'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_confidential_information(text: str) -> str:\n",
    "    \"\"\"Remove confidential information from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text.\n",
    "    Returns:\n",
    "        str: Text with removed confidential information.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b[Xx]{1,}\\b'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_extra_spaces(text: str) -> str:\n",
    "    \"\"\"Remove extra spaces or new lines from a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text.\n",
    "    Returns:\n",
    "        str: Text with removed extra spaces or new lines.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s+'\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"Remove stop words from text\n",
    "\n",
    "    Args:\n",
    "        text (str): Text.\n",
    "    Returns:\n",
    "        str: Text with stop words removed.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    return ' '.join([token for token in tokens if token not in stop_words])\n",
    "\n",
    "# source: https://www.ibm.com/topics/stemming-lemmatization#:~:text=The%20practical%20distinction%20between%20stemming,be%20found%20in%20the%20dictionary.\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    \"\"\"Return wordnet constant value to do lemmatization based on their input word tag\n",
    "\n",
    "    Args:\n",
    "        tag (str): Tag name.\n",
    "    Returns:\n",
    "        str: Constant value for wordnet lemmatization.\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"Perform lemmatization using WordNetLemmatizer\n",
    "\n",
    "    Args:\n",
    "        tokens (str): Text.\n",
    "    Returns:\n",
    "        str: Lemmatized text.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    return ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags])\n",
    "\n",
    "\n",
    "def pad_sequence(tokens: list, max_length: int, post: bool=True) -> np.array:\n",
    "    \"\"\"Perform zero padding before or after the sequence.\n",
    "\n",
    "    Args:\n",
    "        tokens (str): Text.\n",
    "    Returns:\n",
    "        str: Padded sequences.\n",
    "    \"\"\"\n",
    "    padded_tokens = None\n",
    "    if len(tokens) < max_length:\n",
    "        zeros = list(np.zeros(max_length - len(tokens)))\n",
    "        if post:\n",
    "            padded_tokens = tokens + zeros  # Add zeros after the seqeuence\n",
    "        else:\n",
    "            padded_tokens = zeros + tokens  # Add zeros before the seqeuence\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_length]\n",
    "    return padded_tokens\n",
    "\n",
    "def generate_vocabulary(texts: list) -> Vocab:\n",
    "    \"\"\"Generates a vocabulary from a list of texts.\n",
    "\n",
    "    Args:\n",
    "        texts (list): A list of text strings to build the vocabulary from.\n",
    "    Returns:\n",
    "        Vocab: A vocabulary object containing the tokens and their corresponding indices.\n",
    "    \"\"\"\n",
    "    def yield_tokens(texts: list):\n",
    "        for text in texts:\n",
    "            yield tokenizer(text.strip())\n",
    "    # Generate vocabulary\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(texts), min_freq=2, specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def calculate_max_length_sequence(texts: list) -> int:\n",
    "    \"\"\"Calculates the maximum length of token sequences from a list of texts.\n",
    "\n",
    "    Args:\n",
    "        texts (list): Texts.\n",
    "    Returns:\n",
    "        int: The maximum length of the tokenized sequences.\n",
    "    \"\"\"\n",
    "    max_length = 0  \n",
    "    for text in texts:\n",
    "        max_length = max(max_length, len(tokenizer(text)))\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomerComplaintsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset for handling encoded customer complaints and their labels.\n",
    "\n",
    "    Args:\n",
    "        encoded_texts (list): List of encoded text sequences.\n",
    "        encoded_labels (list): List of encoded labels corresponding to the text sequences.\n",
    "        max_length (int): Maximum length of the text sequences.\n",
    "\n",
    "    Attributes:\n",
    "        encoded_texts (list): Stored list of encoded text sequences.\n",
    "        encoded_labels (list): Stored list of encoded labels.\n",
    "        max_length (int): Stored maximum length of the text sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoded_texts: list, encoded_labels: list, max_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.encoded_texts = encoded_texts\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.encoded_labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[list, int]:\n",
    "        \"\"\"sample from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[list, int]: (encoded_text, label) where encoded_text is the encoded text sequence and label is the corresponding label.\n",
    "        \"\"\"\n",
    "        encoded_text = self.encoded_texts[idx]\n",
    "        label = self.encoded_labels[idx]\n",
    "        return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    for text, label in batch:\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    texts, labels = np.array(texts), np.array(labels)\n",
    "    return torch.LongTensor(texts), torch.LongTensor(labels)\n",
    "\n",
    "# Create text preprocessing pipeline\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = remove_confidential_information(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_texts = [preprocess_text(text) for text in X_train[:100]]\n",
    "preprocessed_val_texts = [preprocess_text(text) for text in X_val[:100]]\n",
    "preprocessed_test_texts = [preprocess_text(text) for text in X_test[:100]]\n",
    "\n",
    "vocab = generate_vocabulary(preprocessed_train_texts)\n",
    "max_length = calculate_max_length_sequence(preprocessed_train_texts)\n",
    "\n",
    "train_encoded_texts = [vocab(tokenizer(text)) for text in preprocessed_train_texts]\n",
    "val_encoded_texts = [vocab(tokenizer(text)) for text in preprocessed_val_texts]\n",
    "test_encoded_texts = [vocab(tokenizer(text)) for text in preprocessed_test_texts]\n",
    "\n",
    "train_encoded_texts = [pad_sequence(tokens, max_length) for tokens in train_encoded_texts]\n",
    "val_encoded_texts = [pad_sequence(tokens, max_length) for tokens in val_encoded_texts]\n",
    "test_encoded_texts = [pad_sequence(tokens, max_length) for tokens in test_encoded_texts]\n",
    "\n",
    "train_encoded_labels = [label_to_ix[label] for label in y_train[:100]]\n",
    "val_encoded_labels = [label_to_ix[label] for label in y_val[:100]]\n",
    "test_encoded_labels = [label_to_ix[label] for label in y_test[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomerComplaintsDataset(train_encoded_texts, train_encoded_labels, max_length)\n",
    "val_dataset = CustomerComplaintsDataset(val_encoded_texts, val_encoded_labels, max_length)\n",
    "test_dataset = CustomerComplaintsDataset(test_encoded_texts, test_encoded_labels, max_length)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text\n",
    "processed_texts = list(map(process_text, X_train[:1000]))\n",
    "\n",
    "# Generate vocabulary\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield text.strip().split()\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(processed_texts), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Calculate maximum length\n",
    "max_length = -1  \n",
    "for text in processed_texts:\n",
    "    max_length = max(max_length, len(tokenizer(text)))\n",
    "\n",
    "# Encode texts:\n",
    "encoded_texts = [vocab(tokenizer(text)) for text in processed_texts] \n",
    "\n",
    "# Encode labels:\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    for text, label in batch:\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    texts, labels = np.array(texts), np.array(labels)\n",
    "    return torch.LongTensor(texts), torch.LongTensor(labels)\n",
    "\n",
    "encoded_labels = [encode_label(label) for label in y_train[:1000]]\n",
    "dataset = CustomerComplaintsDataset(encoded_texts, encoded_labels, max_length)\n",
    "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for text classification.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        embedding_size (int): Dimension of the embedding vectors.\n",
    "        num_classes (int): Number of output classes.\n",
    "        max_length (int): Maximum length of the input text sequences.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): Embedding layer to convert input tokens to embeddings.\n",
    "        flatten (nn.Flatten): Layer to flatten the embeddings.\n",
    "        linear (nn.Linear): Linear layer for classification.\n",
    "        log_softmax (nn.LogSoftmax): Log softmax layer for output normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_size: int, num_classes: int, max_length: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(embedding_size * max_length, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            text (torch.Tensor): Input tensor containing tokenized text sequences.\n",
    "        Returns:\n",
    "            torch.Tensor: Log softmax probabilities of the classes.\n",
    "        \"\"\"\n",
    "        model = torch.nn.Sequential(\n",
    "            self.embedding, \n",
    "            self.flatten, \n",
    "            self.linear,\n",
    "            self.log_softmax\n",
    "        )\n",
    "        return model(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Recall\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "num_classes = len(df['topic'].unique())\n",
    "multiclass_acc = Accuracy(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "multiclass_recall = Recall(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "multiclass_precision = Precision(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "multiclass_f1_score = F1Score(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "INTERVAL = 20\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Change model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    total_count = 0\n",
    "    total_acc = 0\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for idx, (texts, labels) in enumerate(dataloader):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Produce model output\n",
    "        predicted_labels = model(texts)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predicted_labels, labels)\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate performance\n",
    "        acc = multiclass_acc(predicted_labels.argmax(1), labels)\n",
    "        recall = multiclass_recall(predicted_labels.argmax(1), labels)\n",
    "        precision = multiclass_precision(predicted_labels.argmax(1), labels)\n",
    "        f1_score = multiclass_f1_score(predicted_labels.argmax(1), labels)\n",
    "        \n",
    "        total_acc += acc.item()\n",
    "        total_recall += recall.item()\n",
    "        total_precision += precision.item()\n",
    "        total_f1_score += f1_score.item()\n",
    "        \n",
    "        # Count batch\n",
    "        total_count += 1\n",
    "        \n",
    "        # Display model training status\n",
    "        if idx % INTERVAL == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| {:5d}/{:5d} batches | accuracy {:.3f} | recall {:.3f} | precision {:.3f} | f1 score {:.3f}\".format(\n",
    "                    idx, len(dataloader), \n",
    "                    total_acc / total_count, \n",
    "                    total_recall / total_count, \n",
    "                    total_precision / total_count, \n",
    "                    total_f1_score / total_count\n",
    "                )\n",
    "            )\n",
    "            total_count = 0\n",
    "            total_acc = 0\n",
    "            total_recall = 0\n",
    "            total_precision = 0\n",
    "            total_f1_score = 0\n",
    "\n",
    "def evaluate(model, dataloader) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Evaluates the model on the provided validation dataset (usually).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the evaluation data.\n",
    "    Returns:\n",
    "        tuple: accuracy, recall, precision, and f1_score of the model on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    # Change model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_count = 0\n",
    "    total_acc = 0\n",
    "    total_recall = 0\n",
    "    total_precision = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            # Produce model output\n",
    "            predicted_labels = model(texts)\n",
    "\n",
    "            # Evaluate performance\n",
    "            acc = multiclass_acc(predicted_labels.argmax(1), labels)\n",
    "            recall = multiclass_recall(predicted_labels.argmax(1), labels)\n",
    "            precision = multiclass_precision(predicted_labels.argmax(1), labels)\n",
    "            f1_score = multiclass_f1_score(predicted_labels.argmax(1), labels)\n",
    "            \n",
    "            total_acc += acc.item()\n",
    "            total_recall += recall.item()\n",
    "            total_precision += precision.item()\n",
    "            total_f1_score += f1_score.item()\n",
    "            \n",
    "            # Count batch\n",
    "            total_count += 1\n",
    "        \n",
    "    return (\n",
    "        total_acc / total_count, \n",
    "        total_recall / total_count,\n",
    "        total_precision / total_count,\n",
    "        total_f1_score / total_count\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================================================\n",
      "| end of epoch   0 | time:  0.25s | valid accuracy    0.233 | valid recall 0.233 | valid precision 0.081 | valid f1 score 0.116\n",
      "==================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare model hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 128\n",
    "num_classes = len(df['topic'].unique())\n",
    "\n",
    "# Create model\n",
    "model = Net(vocab_size, embedding_size, num_classes, max_length)\n",
    "\n",
    "lr = 1e-3\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    # Train:\n",
    "    train(model, train_dataloader, optimizer, criterion)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_accuracy, val_recall, val_precision, val_f1_score = evaluate(model, val_dataloader)\n",
    "    print(\"=\" * 130)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | valid recall {:.3f} | \"\n",
    "        \"valid precision {:.3f} | valid f1 score {:.3f}\".format(\n",
    "            epoch, time.time() - epoch_start_time,\n",
    "            val_accuracy, val_recall,\n",
    "            val_precision, val_f1_score \n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.150 | test recall 0.150 | test precision 0.025 | test f1 score 0.041\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_recall, test_precision, test_f1_score = evaluate(model, test_dataloader)\n",
    "print(\n",
    "    \"test accuracy {:8.3f} | test recall {:.3f} | \"\n",
    "    \"test precision {:.3f} | test f1 score {:.3f}\".format(\n",
    "        test_accuracy, test_recall,\n",
    "        test_precision, test_f1_score \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I paid a company named XXXX XXXX XXXX a total of {$14000.00} over several cards and transactions for a custom e-commerce website and coaching services. They made all kind of promises of income between $ XXXX- $ XXXX a year. It has been over a year and I find that I have not made a penny. Besides that the coaching did n\\'t exist and the \" custom \\'\\' site was nothing more than a substandard, rudimentary, cookie cutter that was never completed. They never finished the logo, never provided any marketing or the coding thumb drive as promised. They ripped me off and I am hoping that someone at my bank can help me get my money back. This company is still operating and I would like to dispute these charges. Services were not rendered and what I received was not what was described. I will be reporting them to the FTC as they are clearly violating the laws and need to be shut down. Please help me.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Theft/Dispute Reporting'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_test = X_test[6]\n",
    "display(sample_test)\n",
    "sample_label = y_test[6]\n",
    "display(sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str, vocab: Vocab, model: nn.Module, max_length: int) -> int:\n",
    "    \"\"\"\n",
    "    Predicts the class label for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to be classified.\n",
    "        vocab (Vocab): Vocabulary object used to encode the text.\n",
    "        model (nn.Module): Trained neural network model for prediction.\n",
    "        max_length (int): Maximum length of the input text sequences.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted class label.\n",
    "    \"\"\"\n",
    "    # Preprocess the input text:\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    # Encode the text using the vocabulary:\n",
    "    encoded_text = vocab(tokenizer(text))\n",
    "    \n",
    "    # Pad the encoded text to the maximum length:\n",
    "    encoded_text = pad_sequence(encoded_text, max_length)\n",
    "    \n",
    "    # Convert the encoded text to a tensor and add a batch dimension:\n",
    "    encoded_text = torch.LongTensor(encoded_text).unsqueeze(0)\n",
    "    \n",
    "    # Predict the class label using the model:\n",
    "    output = model(encoded_text).argmax(1)\n",
    "    \n",
    "    return output.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Credit card or prepaid card'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_label[predict(sample_test, vocab, model, max_length)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-complaints-classification-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
